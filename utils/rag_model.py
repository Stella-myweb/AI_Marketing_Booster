# utils/rag_model.py
import os
import streamlit as st
from typing import Dict, List, Any, Optional

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import TextLoader

# ì„¤ì •
from config import OPENAI_API_KEY, LLM_MODEL, TEMPERATURE, DATA_DIR

# ---------------------- ë²¡í„°ìŠ¤í† ì–´ ----------------------
class VectorStore:
    def __init__(self):
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", OPENAI_API_KEY)
            if not api_key:
                st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                raise ValueError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # API í‚¤ ìœ íš¨ì„± ê²€ì‚¬
            try:
                self.embeddings = OpenAIEmbeddings(
                    openai_api_key=api_key,
                    model="text-embedding-ada-002"
                )
                # ê°„ë‹¨í•œ ì„ë² ë”© í…ŒìŠ¤íŠ¸
                self.embeddings.embed_query("test")
            except Exception as e:
                st.error(f"OpenAI API í‚¤ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
                raise ValueError("API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            self.data_dir = DATA_DIR
            vectorstore_path = os.path.join(self.data_dir, "vectorstore")
            
            # ë²¡í„°ìŠ¤í† ì–´ ë””ë ‰í† ë¦¬ í™•ì¸
            if not os.path.exists(self.data_dir):
                os.makedirs(self.data_dir, exist_ok=True)
                st.warning(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {self.data_dir}")
            
            if os.path.exists(vectorstore_path):
                try:
                    self.vectorstore = FAISS.load_local(vectorstore_path, self.embeddings)
                    st.success("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.warning(f"ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤: {e}")
                    self._create_vectorstore()
            else:
                st.info("ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
                self._create_vectorstore()
                
        except Exception as e:
            st.error(f"ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            raise

    def _create_vectorstore(self):
        try:
            content_dir = os.path.join(self.data_dir, "content")
            if not os.path.exists(content_dir):
                os.makedirs(content_dir, exist_ok=True)
                st.info(f"ì½˜í…ì¸  ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {content_dir}")
            
            text_files = [os.path.join(root, file)
                         for root, _, files in os.walk(content_dir)
                         for file in files if file.endswith(".txt")]
            
            ebook_file = os.path.join(self.data_dir, "ebook_content.txt")
            if os.path.exists(ebook_file):
                text_files.append(ebook_file)
            
            if not text_files:
                default_file = os.path.join(content_dir, "default_content.txt")
                with open(default_file, "w", encoding="utf-8") as f:
                    f.write("""
                    ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™”ë¥¼ ìœ„í•œ ê¸°ë³¸ ê°€ì´ë“œ:
                    1. ì •í™•í•œ ê¸°ë³¸ ì •ë³´ ì…ë ¥í•˜ê¸°
                    2. ë§¤ë ¥ì ì¸ ì´ë¯¸ì§€ ì‚¬ìš©í•˜ê¸°
                    3. í‚¤ì›Œë“œ ìµœì í™”í•˜ê¸°
                    4. ê³ ê° ë¦¬ë·° ê´€ë¦¬í•˜ê¸°
                    5. ì •ê¸°ì ì¸ ì—…ë°ì´íŠ¸í•˜ê¸°
                    """)
                text_files.append(default_file)
                st.info("ê¸°ë³¸ ì½˜í…ì¸  íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            documents = []
            for file_path in text_files:
                try:
                    loader = TextLoader(file_path, encoding="utf-8")
                    documents.extend(loader.load())
                    st.success(f"íŒŒì¼ ë¡œë“œ ì„±ê³µ: {os.path.basename(file_path)}")
                except Exception as e:
                    st.warning(f"íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜ ({file_path}): {e}")
            
            if not documents:
                raise ValueError("ë¡œë“œí•  ìˆ˜ ìˆëŠ” ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
            split_documents = text_splitter.split_documents(documents)
            
            self.vectorstore = FAISS.from_documents(split_documents, self.embeddings)
            vectorstore_path = os.path.join(self.data_dir, "vectorstore")
            self.vectorstore.save_local(vectorstore_path)
            st.success("ë²¡í„°ìŠ¤í† ì–´ê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            raise

    def get_relevant_content(self, query: str, n_results: int = 3) -> str:
        try:
            docs = self.vectorstore.similarity_search(query, k=n_results)
            context = "\n\n".join([doc.page_content for doc in docs])
            return context
        except Exception as e:
            st.error(f"ì½˜í…ì¸  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return "ì½˜í…ì¸  ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def raw_similarity_search(self, query: str, k: int = 5):
        try:
            if not query:
                return []
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            st.error(f"ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

# ---------------------- ì§ˆë¬¸/ì§„ë‹¨ ìœ í‹¸ë¦¬í‹° (ê°„ëµí™”) ----------------------
# ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œëŠ” questions.pyì—ì„œ import í•˜ê±°ë‚˜, ì•„ë˜ì²˜ëŸ¼ í•„ìš”í•œ í•¨ìˆ˜ë§Œ í¬í•¨

def suggest_improvements(result):
    # ì˜ˆì‹œ: ê°œì„ ì  ì¶”ì²œ (ì‹¤ì œ ë¡œì§ì€ questions.py ì°¸ê³ )
    return ["í‚¤ì›Œë“œ ë‹¤ì–‘í™”", "ì´ë¯¸ì§€ í’ˆì§ˆ ê°œì„ ", "ë¦¬ë·° ê´€ë¦¬ ê°•í™”"]

# ---------------------- RAG ëª¨ë¸ í†µí•© ----------------------
class RAGModel:
    def __init__(self):
        try:
            api_key = st.secrets.get("OPENAI_API_KEY", OPENAI_API_KEY)
            if not api_key:
                st.error("OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                raise ValueError("API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            
            # API í‚¤ ìœ íš¨ì„± ê²€ì‚¬
            try:
                self.llm = ChatOpenAI(
                    openai_api_key=api_key,
                    model_name=LLM_MODEL,
                    temperature=TEMPERATURE
                )
                # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
                self.llm.predict("test")
            except Exception as e:
                st.error(f"OpenAI API í‚¤ ìœ íš¨ì„± ê²€ì‚¬ ì‹¤íŒ¨: {e}")
                raise ValueError("API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
            
            # ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”
            try:
                self.vector_store = VectorStore()
                st.success("RAG ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                raise ValueError("ë²¡í„°ìŠ¤í† ì–´ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
                
        except Exception as e:
            st.error(f"RAG ëª¨ë¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            self.llm = None
            self.vector_store = None
            raise

    def generate_response(self, query: str, context: str = None, n_results: int = 3) -> str:
        """
        ì¿¼ë¦¬ì— ëŒ€í•œ ì „ë¬¸ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. (1000ì ì´ë‚´, ì´ëª¨í‹°ì½˜ ì†Œì œëª©)
        """
        try:
            if not context and self.vector_store:
                context = self.vector_store.get_relevant_content(query, n_results=n_results)
            elif not context:
                context = """
                ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì¼ë°˜ íŒ:
                1. ë§¤ë ¥ì ì¸ ì´ë¯¸ì§€ ì‚¬ìš©í•˜ê¸°
                2. í•µì‹¬ í‚¤ì›Œë“œ í¬í•¨í•˜ê¸°
                3. ìƒì„¸í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ì„¤ëª… ì œê³µí•˜ê¸°
                4. ì •ê¸°ì ì¸ ì½˜í…ì¸  ì—…ë°ì´íŠ¸í•˜ê¸°
                5. ê³ ê° ë¦¬ë·° ê´€ë¦¬í•˜ê¸°
                """
            prompt = f"""
            ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ 1000ì ì´ë‚´ë¡œ, ì‹¤ì œ ì‚¬ë¡€ì™€ í†µê³„, ìµœì‹  íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•˜ì—¬ ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ê° ì†Œì œëª©ì€ ì´ëª¨í‹°ì½˜(ì˜ˆ: # ğŸ“Š, # ğŸ¯, # ğŸ’¡)ìœ¼ë¡œ êµ¬ë¶„í•´ ì£¼ì„¸ìš”.

            ì°¸ê³  ìë£Œ:
            {context}

            ì§ˆë¬¸: {query}

            ë‹µë³€ í˜•ì‹ ì˜ˆì‹œ:
            # ğŸ“Š í˜„í™© ë¶„ì„\n(í˜„í™©)
            # ğŸ¯ í•µì‹¬ ì „ëµ\n(ì „ëµ)
            # ğŸ’¡ ì‹¤ì „ íŒ\n(íŒ)

            ë‹µë³€:
            """
            response = self.llm.predict(prompt)
            return response[:1000]  # 1000ì ì´ë‚´ë¡œ ì œí•œ
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def generate_diagnosis_report(self, answers: Dict[str, str], diagnosis_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        ìê°€ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ì†Œì œëª©ë³„ë¡œ ì „ë¬¸ì  ë¶„ì„ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        try:
            level = diagnosis_result.get("level", {}).get("name", "ê¸°ë³¸")
            improvements = diagnosis_result.get("improvements", {})
            weak_areas = [area['stage'] for area in improvements.get('weak_areas', [])]
            strong_areas = [area['stage'] for area in improvements.get('strong_areas', [])]
            area_contexts = {}
            
            if self.vector_store:
                # ì•½ì  ì˜ì—­ê³¼ ê°•ì  ì˜ì—­ ëª¨ë‘ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
                for area in set(weak_areas + strong_areas):
                    query = f"ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ {area} ì „ëµê³¼ ì„±ê³µ ì‚¬ë¡€"
                    area_contexts[area] = self.vector_store.get_relevant_content(query, n_results=2)
            
            title_map = {
                "ì¸ì‹í•˜ê²Œ í•œë‹¤": "ê²€ìƒ‰ ë…¸ì¶œ ìµœì í™”",
                "í´ë¦­í•˜ê²Œ í•œë‹¤": "í´ë¦­ìœ¨ ë†’ì´ëŠ” ì „ëµ",
                "ë¨¸ë¬¼ê²Œ í•œë‹¤": "ì²´ë¥˜ì‹œê°„ ëŠ˜ë¦¬ëŠ” ë°©ë²•",
                "ì—°ë½ì˜¤ê²Œ í•œë‹¤": "ë¬¸ì˜/ì˜ˆì•½ ì „í™˜ìœ¨ ë†’ì´ê¸°",
                "í›„ì† í”¼ë“œë°± ë°›ëŠ”ë‹¤": "ê³ ê° ì¬ë°©ë¬¸ ìœ ë„ ì „ëµ"
            }

            # ê° ì†Œì œëª©ë³„ë¡œ ë”°ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompts = {
                "overview": f"""
                ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ\n# ğŸ“Š ì¢…í•© ì§„ë‹¨\n
                1. í˜„ì¬ ìƒíƒœ: {level} ë ˆë²¨ë¡œ ì§„ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.
                2. ê°•ì  ì˜ì—­: {', '.join([title_map.get(area, area) for area in strong_areas[:2]])}
                3. ê°œì„  ì˜ì—­: {', '.join([title_map.get(area, area) for area in weak_areas[:2]])}
                
                800ì ì´ë‚´ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ì¢…í•©ì ì¸ ì§„ë‹¨ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
                - í˜„ì¬ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìš´ì˜ì˜ ì „ë°˜ì ì¸ ìˆ˜ì¤€
                - ê°•ì  ì˜ì—­ì—ì„œì˜ ìš°ìˆ˜í•œ ì 
                - ê°œì„  ì˜ì—­ì—ì„œì˜ ì£¼ìš” ê³¼ì œ
                - í–¥í›„ ë°œì „ ë°©í–¥
                
                ì°¸ê³  ìë£Œ:
                {'\\n'.join([area_contexts.get(area, '') for area in weak_areas[:2] + strong_areas[:2]])}
                """,
                
                "strengths_analysis": f"""
                ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ\n# ğŸ’ª ê°•ì  ë¶„ì„\n
                1. ê°•ì  ì˜ì—­: {', '.join([title_map.get(area, area) for area in strong_areas[:2]])}
                2. ì§„ë‹¨ ë ˆë²¨: {level}
                
                800ì ì´ë‚´ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ê°•ì  ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
                - ê° ê°•ì  ì˜ì—­ë³„ ì„¸ë¶€ ë¶„ì„
                - í˜„ì¬ ì˜ í•˜ê³  ìˆëŠ” ì 
                - ê°•ì ì„ ë”ìš± ê°•í™”í•  ìˆ˜ ìˆëŠ” ë°©ì•ˆ
                - ê²½ìŸì‚¬ ëŒ€ë¹„ ìš°ìœ„ ìš”ì†Œ
                
                ì°¸ê³  ìë£Œ:
                {'\\n'.join([area_contexts.get(area, '') for area in strong_areas[:2]])}
                """,
                
                "improvements_analysis": f"""
                ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ\n# ğŸ¯ ê°œì„ ì  ë¶„ì„\n
                1. ê°œì„  ì˜ì—­: {', '.join([title_map.get(area, area) for area in weak_areas[:2]])}
                2. ì§„ë‹¨ ë ˆë²¨: {level}
                
                800ì ì´ë‚´ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ê°œì„ ì  ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
                - ê° ê°œì„  ì˜ì—­ë³„ ì„¸ë¶€ ë¶„ì„
                - í˜„ì¬ ë¶€ì¡±í•œ ì 
                - ê°œì„ ì´ í•„ìš”í•œ ì´ìœ 
                - ê°œì„  ì‹œ ê¸°ëŒ€ íš¨ê³¼
                
                ì°¸ê³  ìë£Œ:
                {'\\n'.join([area_contexts.get(area, '') for area in weak_areas[:2]])}
                """,
                
                "action_plan": f"""
                ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ\n# ğŸ“ ì•¡ì…˜ í”Œëœ\n
                1. ê°œì„  ì˜ì—­: {', '.join([title_map.get(area, area) for area in weak_areas[:2]])}
                2. ì§„ë‹¨ ë ˆë²¨: {level}
                
                800ì ì´ë‚´ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ êµ¬ì²´ì ì¸ ì•¡ì…˜ í”Œëœì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
                - ë‹¨ê¸° ì‹¤í–‰ ê³„íš (1-2ì£¼)
                - ì¤‘ê¸° ì‹¤í–‰ ê³„íš (1-3ê°œì›”)
                - ì¥ê¸° ì‹¤í–‰ ê³„íš (3-6ê°œì›”)
                - ê° ë‹¨ê³„ë³„ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆ
                - ì˜ˆìƒë˜ëŠ” ê²°ê³¼ì™€ íš¨ê³¼
                
                ì°¸ê³  ìë£Œ:
                {'\\n'.join([area_contexts.get(area, '') for area in weak_areas[:2]])}
                """,
                
                "upgrade_tips": f"""
                ë‹¹ì‹ ì€ ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì§„ë‹¨ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ\n# ğŸ’¡ ê³ ê¸‰ ì „ëµ íŒ\n
                1. ì§„ë‹¨ ë ˆë²¨: {level}
                2. ê°•ì  ì˜ì—­: {', '.join([title_map.get(area, area) for area in strong_areas[:2]])}
                3. ê°œì„  ì˜ì—­: {', '.join([title_map.get(area, area) for area in weak_areas[:2]])}
                
                800ì ì´ë‚´ë¡œ ë‹¤ìŒ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ê³ ê¸‰ ì „ëµ íŒì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
                - ê²½ìŸì‚¬ì™€ì˜ ì°¨ë³„í™” ì „ëµ
                - ìµœì‹  íŠ¸ë Œë“œ í™œìš© ë°©ì•ˆ
                - ê³ ê° ê²½í—˜ í–¥ìƒ íŒ
                - ROIë¥¼ ë†’ì´ëŠ” ì‹¤ì „ ì „ëµ
                
                ì°¸ê³  ìë£Œ:
                {'\\n'.join([area_contexts.get(area, '') for area in weak_areas[:2] + strong_areas[:2]])}
                """
            }
            
            results = {}
            for key, prompt in prompts.items():
                results[key] = self.llm.predict(prompt)[:800]
            
            return {
                "title": "ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ëµ ê°€ì´ë“œ",
                "level": level,
                "overview": results["overview"],
                "strengths_analysis": results["strengths_analysis"],
                "improvements_analysis": results["improvements_analysis"],
                "action_plan": results["action_plan"],
                "upgrade_tips": results["upgrade_tips"]
            }
        except Exception as e:
            st.error(f"ì§„ë‹¨ ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "title": "ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ í”Œë ˆì´ìŠ¤ ìµœì í™” ì „ëµ ê°€ì´ë“œ",
                "level": diagnosis_result.get("level", {}).get("name", "ê¸°ë³¸"),
                "overview": "ì§„ë‹¨ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "strengths_analysis": "ì§„ë‹¨ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "improvements_analysis": "ì§„ë‹¨ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "action_plan": "ì§„ë‹¨ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "upgrade_tips": "ì§„ë‹¨ ê²°ê³¼ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
            }

    def search_ebook_content(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        try:
            if self.vector_store is None:
                return [{"content": "ì´ë¶ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "source": "ì‹œìŠ¤í…œ"}]
            docs = self.vector_store.raw_similarity_search(query, n_results)
            results = []
            for i, doc in enumerate(docs):
                results.append({
                    "content": doc.page_content,
                    "source": doc.metadata.get("source", f"ì´ë¶ ì„¹ì…˜ {i+1}"),
                    "relevance": round((1.0 - (i * 0.1)), 2)
                })
            return results
        except Exception as e:
            st.error(f"ì´ë¶ ì½˜í…ì¸  ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return [{"content": "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", "source": "ì‹œìŠ¤í…œ"}]

__all__ = ['RAGModel'] 
